# Experiment: ResNet18 on FaceForensics++ (FFPP)

# -----------------------------------------------------------------------------------------------------
# General Settings
# -----------------------------------------------------------------------------------------------------
project_name: "deepfake-detection"
seed: 42

# -----------------------------------------------------------------------------------------------------
# Data Configuration (for DeepfakeDataModule)
# -----------------------------------------------------------------------------------------------------
data:
  data_dir: data_frames/ffpp          
  label_dirs:
    REAL: 0
    FAKE: 1
  load_sequences: True           # Default: false (loads single frames [C,H,W])
  sequence_length: 16             # Used if load_sequences: true (sequence length T)
  sampling_stride: 1              # Used if load_sequences: true (stride between frames in sequence)
  max_frames_per_video: null      # Optional: max frames per video (for SingleFrameDataset)
  max_videos_per_split: null      # Optional: max videos per split (for SequenceDataset)

  # DataLoader parameters
  batch_size: 2
  num_workers: 4

  # Parameters for building transforms
  transform:
    img_size: [1080,720]  # [H,W]
    norm_mean: [0.485, 0.456, 0.406]
    norm_std: [0.229, 0.224, 0.225]
    train_augmentations: true

# -----------------------------------------------------------------------------------------------------
# Model Configuration (for DeepfakeClassifier)
# -----------------------------------------------------------------------------------------------------
model:
  model_name: VideoTransformer
  num_classes: 2

  model_params:
    image_size: [1080, 720]  # [H,W]
    num_frames: 16
    in_channels: 3
    patch_size: [2, 16, 16] # [T, H, W]
    embed_dim: 768
    depth: 12
    num_heads: 12
    mlp_ratio: 4.0
    dropout: 0.1
    num_classes: 2

  # Parameters for criterion
  criterion_name: "cross_entropy"
  criterion_params: {
    weight: [4.7, 0.56]
  }

    
  # Parameters for optimizer and scheduler
  optimizer_name: "adam"
  optimizer_params:
    weight_decay: 0.01
    lr: 0.0001
    betas: [0.9, 0.999]
    eps: 0.0000001

  use_scheduler: true
  scheduler_name: "warmup"
  scheduler_params:
    num_warmup_steps: 500    # e.g. first 500 steps warm up
    # num_training_steps=total_steps = num_epochs * (num_training_samples // batch_size)

  accuracy_task: "multiclass"
  accuracy_task_params:
    num_classes: 2 # Only used for multiclass accuracy metric

# -----------------------------------------------------------------------------------------------------
# Settings for the PyTorch Lightning Trainer
# -----------------------------------------------------------------------------------------------------
trainer:
  accelerator: "auto"
  devices: 4
  precision: bf16-mixed
  deterministic: false
  max_epochs: 250
  callbacks:
    early_stopping:
      patience: 10

# -----------------------------------------------------------------------------------------------------
# Logging Configuration (for the Trainer's logger)
# -----------------------------------------------------------------------------------------------------
logger:
  type: "tensorboard"

# -----------------------------------------------------------------------------
# Settings for the execution
# -----------------------------------------------------------------------------

output_dir: "./outputs"
