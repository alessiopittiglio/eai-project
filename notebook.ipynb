{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac48c62",
   "metadata": {},
   "source": [
    "# GLOBAL VARIABLE DEFINITION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f8739",
   "metadata": {},
   "source": [
    "To run the script, create the model class inside \n",
    "\n",
    "```\n",
    "models/MODEL_NAME/model.py \n",
    "```\n",
    "\n",
    "The module will be dynamically imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4328ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment for a model training\n",
    "MODEL_NAME = \"Xception3d\"\n",
    "MODEL_PATH = \"models/\" + MODEL_NAME + \"/\"\n",
    "MODEL_PATH_METRICS = MODEL_PATH + \"metrics/\"\n",
    "\n",
    "# Set the data paths\n",
    "DATASET = \"FFPP\" # Dataset name, values can be [\"DFDC\",\"FFPP\"]\n",
    "\n",
    "# IF FFPP, set the following paths for reorganize the data\n",
    "ORIGIN_ROOT = \"data/ff++/original_sequences/actors/c23/videos\"\n",
    "MANIP_ROOT= \"data/ff++/manipulated_sequences/DeepFakeDetection/c23/videos\"\n",
    "OUTPUT_ROOT = \"data/ff++/\" # It will save the reorganized data here, train, val, test\n",
    "\n",
    "# IF DFDC, set the following paths\n",
    "METADATA_PATH = './data/metadata.json'\n",
    "\n",
    "DATA_TRAIN_PATH = './data/ff++/train'\n",
    "DATA_VAL_PATH = './data/ff++/val'\n",
    "DATA_TEST_PATH = './data/ff++/test' # Change this to the test data path if available\n",
    "\n",
    "# SET THE CROSS VALIDATION PARAMETERS\n",
    "CROSS_VAL = False # Set to True for cross-validation\n",
    "if CROSS_VAL:\n",
    "    NUM_SPLITS = 5 # Number of splits for cross-validation\n",
    "    \n",
    "# Set the training parameters\n",
    "BATCH_SIZE_TRAIN = 4\n",
    "BATCH_SIZE_VAL = 4\n",
    "BATCH_SIZE_TEST = 4\n",
    "\n",
    "NUM_EPOCHS = 10 # Number of epochs to train\n",
    "PATIENCE = 5  # Number of epochs to wait for improvement\n",
    "\n",
    "# Set the training parameters\n",
    "BEST_MODEL_PATH = MODEL_PATH+'best_model.pth'\n",
    "LAST_MODEL_PATH = MODEL_PATH+'last_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if MODEL_PATH folder exists\n",
    "import os\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model directory '{MODEL_PATH}' does not exist. Please create it before running the script.\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH_METRICS):\n",
    "    os.makedirs(MODEL_PATH_METRICS)\n",
    "    print(f\"Created directory '{MODEL_PATH_METRICS}' for metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a579af",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# Numerical and data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# PyTorch utilities\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Machine learning and evaluation\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d221a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the module path dynamically\n",
    "module_path = f\"models.{MODEL_NAME}.model\"\n",
    "\n",
    "# Import the module\n",
    "model_module = importlib.import_module(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784672a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1c5fa",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open metadata.json file\n",
    "with open(METADATA_PATH) as f:\n",
    "    metadata = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the metadata to a pandas DataFrame\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "\n",
    "# Transpose the DataFrame\n",
    "metadata_df = metadata_df.transpose()\n",
    "\n",
    "# Reset the index\n",
    "metadata_df.reset_index(inplace=True)\n",
    "\n",
    "# Rename the column index to filename\n",
    "metadata_df.rename(columns={'index': 'filename'}, inplace=True)\n",
    "\n",
    "# Convert labels to integers.\n",
    "# REAL = 0\n",
    "# FAKE = 1\n",
    "metadata_df['label'] = metadata_df['label'].map({'REAL': 0, 'FAKE': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2956bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2d2b5",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=16):\n",
    "    \"\"\"Extract `num_frames` evenly spaced frames from a video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idxs = list(np.linspace(0, length-1, num_frames).astype(int))\n",
    "    frames = []\n",
    "    for idx in frame_idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, video_dir, labels_df, transform=None, num_frames=16):\n",
    "        self.video_dir = video_dir\n",
    "        \n",
    "        # Remove all the labels that are not in the video_dir\n",
    "        labels_df = labels_df[labels_df['filename'].isin(os.listdir(video_dir))]\n",
    "        \n",
    "        self.labels = labels_df\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.labels.iloc[idx]['filename']\n",
    "        label = self.labels.iloc[idx]['label']  # 0: real, 1: fake\n",
    "        video_path = os.path.join(self.video_dir, video_file)\n",
    "        frames = extract_frames(video_path, num_frames=self.num_frames)\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "        # stack to (num_frames, C, H, W)\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def get_filenames(self):\n",
    "        return self.labels['filename'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdeb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ffpp_splits(\n",
    "    orig_root: str,\n",
    "    manip_root: str,\n",
    "    output_root: str,\n",
    "    val_size: float = 0.15,\n",
    "    test_size: float = 0.15,\n",
    "    random_state: int = 42,\n",
    "    use_symlinks: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Gather all videos from orig_root (label=0) and manip_root (label=1)\n",
    "    2. Split into train/val/test with stratification on label\n",
    "    3. Copy or symlink them into output_root/{train,val,test}/{real,fake}/\n",
    "    \"\"\"\n",
    "    # 1) gather\n",
    "    orig_paths = glob.glob(os.path.join(orig_root, \"*.mp4\"))\n",
    "    manip_paths = glob.glob(os.path.join(manip_root, \"*.mp4\"))\n",
    "    records = []\n",
    "    for p in orig_paths:\n",
    "        records.append({\"filename\": os.path.basename(p), \"label\": 0, \"path\": p})\n",
    "    for p in manip_paths:\n",
    "        records.append({\"filename\": os.path.basename(p), \"label\": 1, \"path\": p})\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # 2) split\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=(val_size + test_size), \n",
    "        stratify=df[\"label\"], random_state=random_state\n",
    "    )\n",
    "    # split temp into val/test proportional\n",
    "    rel_val = val_size / (val_size + test_size)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, test_size=(1 - rel_val),\n",
    "        stratify=temp_df[\"label\"], random_state=random_state\n",
    "    )\n",
    "\n",
    "    splits = {\"train\": train_df, \"val\": val_df, \"test\": test_df}\n",
    "\n",
    "    # 3) copy or symlink\n",
    "    for split_name, split_df in splits.items():\n",
    "        for _, row in split_df.iterrows():\n",
    "            lbl_folder = \"real\" if row[\"label\"] == 0 else \"fake\"\n",
    "            dest_dir = os.path.join(output_root, split_name, lbl_folder)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            dest_path = os.path.join(dest_dir, row[\"filename\"])\n",
    "            if use_symlinks:\n",
    "                if os.path.exists(dest_path):\n",
    "                    os.remove(dest_path)\n",
    "                os.symlink(os.path.abspath(row[\"path\"]), dest_path)\n",
    "            else:\n",
    "                # copy only if not already there\n",
    "                if not os.path.exists(dest_path):\n",
    "                    shutil.copy2(row[\"path\"], dest_path)\n",
    "\n",
    "    print(\"Done! Splits created under\", output_root)\n",
    "\n",
    "class FFPPVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Standard torch Dataset for one split of FF++:\n",
    "       root/\n",
    "         train/ or val/ or test/\n",
    "           real/  *.mp4\n",
    "           fake/  *.mp4\n",
    "    \"\"\"\n",
    "    def __init__(self, split_root, transform=None, num_frames=16):\n",
    "        \"\"\"\n",
    "        split_root: e.g. \"data/train\"\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # find all videos and labels\n",
    "        self.samples = []\n",
    "        for label, sub in enumerate([\"real\", \"fake\"]):\n",
    "            folder = os.path.join(split_root, sub)\n",
    "            for vid in os.listdir(folder):\n",
    "                if vid.lower().endswith(\".mp4\"):\n",
    "                    self.samples.append({\n",
    "                        \"path\": os.path.join(folder, vid),\n",
    "                        \"label\": label\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        frames = extract_frames(sample[\"path\"], num_frames=self.num_frames)\n",
    "        if self.transform:\n",
    "            frames = [self.transform(f) for f in frames]\n",
    "        frames = torch.stack(frames)            # (num_frames, C, H, W)\n",
    "        label = torch.tensor(sample[\"label\"], dtype=torch.long)\n",
    "        return frames, label\n",
    "\n",
    "    def get_filenames(self):\n",
    "        return [os.path.basename(s[\"path\"]) for s in self.samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7738482",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = model_module.get_data_transform()\n",
    "\n",
    "if DATASET == \"DFDC\":\n",
    "    # Create datasets for training & validation, download if necessary\n",
    "    training_set = DeepfakeDataset(DATA_TRAIN_PATH, metadata_df, transform=data_transforms)\n",
    "    validation_set = DeepfakeDataset(DATA_VAL_PATH, metadata_df, transform=data_transforms)\n",
    "    test_set = DeepfakeDataset(DATA_TEST_PATH, metadata_df, transform=data_transforms)\n",
    "elif DATASET == \"FFPP\":\n",
    "    # 1. Prepare splits (run once)\n",
    "    prepare_ffpp_splits(\n",
    "        orig_root=ORIGIN_ROOT,\n",
    "        manip_root=MANIP_ROOT,\n",
    "        output_root=\"/\".join(DATA_TRAIN_PATH.split('/')[:-1]),\n",
    "        val_size=0.15,\n",
    "        test_size=0.15,\n",
    "        use_symlinks=True  # or True, if you prefer\n",
    "    )\n",
    "\n",
    "    # 2. Instantiate datasets\n",
    "    training_set = FFPPVideoDataset(DATA_TRAIN_PATH, transform=data_transforms)\n",
    "    validation_set   = FFPPVideoDataset(DATA_VAL_PATH,   transform=data_transforms)\n",
    "    test_set  = FFPPVideoDataset(DATA_TEST_PATH,  transform=data_transforms)\n",
    "\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "# If CROSS_VAL is set to True, the loaders will be created in the cross-validation loop\n",
    "if not CROSS_VAL:\n",
    "    training_loader = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE_VAL, shuffle=False)\n",
    "\n",
    "# Create data loader for test set\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('FAKE', 'REAL')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22bad1c",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14248b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    # img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42999b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid([images[0][0], images[1][0], images[2][0], images[3][0]], nrow=4)\n",
    "matplotlib_imshow(img_grid)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f1ef5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0706b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now can access things inside model_module\n",
    "model = model_module.Model()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print (\"Using device:\", device)\n",
    "\n",
    "# Multi-GPU support\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeef8ca",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = model_module.get_loss_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda732cc",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = model_module.get_optimizer(model.parameters())\n",
    "scheduler = None\n",
    "\n",
    "if isinstance(optimizer, tuple):\n",
    "    optimizer, scheduler = optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4e3d1",
   "metadata": {},
   "source": [
    "# Save all the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_value(value):\n",
    "    \"\"\"Helper to safely serialize values for JSON.\"\"\"\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return value.tolist()\n",
    "    elif isinstance(value, dict):\n",
    "        return {k: serialize_value(v) for k, v in value.items()}\n",
    "    elif isinstance(value, set):\n",
    "        return list(value)\n",
    "    else:\n",
    "        return str(value)\n",
    "\n",
    "def extract_params(obj):\n",
    "    \"\"\"Extracts public parameters from an object's __dict__, serializing them.\"\"\"\n",
    "    params = {}\n",
    "    for key, value in getattr(obj, '__dict__', {}).items():\n",
    "        if key.startswith('_'):\n",
    "            continue  # skip private attributes\n",
    "        params[key] = serialize_value(value)\n",
    "    return params\n",
    "\n",
    "def get_transform_info(transform):\n",
    "    \"\"\"Extracts transform name and parameters.\"\"\"\n",
    "    return {\n",
    "        'name': transform.__class__.__name__,\n",
    "        'params': extract_params(transform)\n",
    "    }\n",
    "\n",
    "# --- LOSS CONFIG ---\n",
    "loss_config = extract_params(loss_fn)\n",
    "\n",
    "# --- OPTIMIZER CONFIG ---\n",
    "optimizer_params = serialize_value(optimizer.defaults)\n",
    "\n",
    "# --- SCHEDULER CONFIG ---\n",
    "if scheduler is not None:\n",
    "    scheduler_params = serialize_value(scheduler.__dict__)\n",
    "    scheduler_config = {\n",
    "        'scheduler': scheduler.__class__.__name__,\n",
    "        'params': scheduler_params\n",
    "    }\n",
    "else:\n",
    "    scheduler_config = None\n",
    "\n",
    "# --- DATA TRANSFORMS CONFIG ---\n",
    "if hasattr(data_transforms, 'transforms'):\n",
    "    transform_list = [get_transform_info(t) for t in data_transforms.transforms]\n",
    "else:\n",
    "    transform_list = [get_transform_info(data_transforms)]\n",
    "\n",
    "# --- FINAL MODEL INFO ---\n",
    "model_info = {\n",
    "    'model': MODEL_NAME,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'optimizer_params': optimizer_params,\n",
    "    'scheduler': scheduler_config,\n",
    "    'scheduler_params': scheduler_params if scheduler is not None else None,\n",
    "    'loss_fn': loss_fn.__class__.__name__,\n",
    "    'loss_fn_params': loss_config,\n",
    "    'data_transforms': transform_list,\n",
    "    'dataset': DATASET,\n",
    "    'training_set_size': len(training_set),\n",
    "    'validation_set_size': len(validation_set),\n",
    "    'test_set_size': len(test_set),\n",
    "    'batch_size_train': BATCH_SIZE_TRAIN,\n",
    "    'batch_size_val': BATCH_SIZE_VAL,\n",
    "    'batch_size_test': BATCH_SIZE_TEST,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'patience': PATIENCE,\n",
    "    'best_model_path': BEST_MODEL_PATH,\n",
    "    'last_model_path': LAST_MODEL_PATH,\n",
    "    'cross_val': CROSS_VAL,\n",
    "    'num_splits': NUM_SPLITS if CROSS_VAL else None,\n",
    "    'date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "# --- SAVE JSON ---\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "with open(os.path.join(MODEL_PATH, 'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=4)\n",
    "\n",
    "print(\"Model info saved successfully âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc3817",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model for one epoch\n",
    "def train_epoch(model, loader, criterion, optimizer, writer, epoch):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        loader: DataLoader for the training data.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer for updating model weights.\n",
    "        writer: TensorBoard SummaryWriter for logging.\n",
    "        epoch: Current epoch number.\n",
    "\n",
    "    Returns:\n",
    "        epoch_loss: Average loss for the epoch.\n",
    "        epoch_acc: Accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        # Move inputs and labels to the device (GPU/CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1)) # Ensure labels are of shape (batch_size, 1)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs >= 0.5).long().squeeze(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    # Scheduler step if using a scheduler\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "        print(f\"Learning rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    # Log training loss and accuracy to TensorBoard\n",
    "    writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
    "    writer.add_scalar('Training Accuracy', epoch_acc, epoch)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Function to evaluate the model for one epoch\n",
    "def eval_epoch(model, loader, criterion, writer, epoch):\n",
    "    \"\"\"\n",
    "    Evaluate the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        loader: DataLoader for the validation data.\n",
    "        criterion: Loss function.\n",
    "        writer: TensorBoard SummaryWriter for logging.\n",
    "        epoch: Current epoch number.\n",
    "\n",
    "    Returns:\n",
    "        epoch_loss: Average loss for the epoch.\n",
    "        epoch_acc: Accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in tqdm(loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            # Move inputs and labels to the device (GPU/CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1)) # Ensure labels are of shape (batch_size, 1)\n",
    "\n",
    "            # Accumulate loss and accuracy\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= 0.5).long().squeeze(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    # Log validation loss and accuracy to TensorBoard\n",
    "    writer.add_scalar('Validation Loss', epoch_loss, epoch)\n",
    "    writer.add_scalar('Validation Accuracy', epoch_acc, epoch)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dcbe0",
   "metadata": {},
   "source": [
    "# Per-epoch activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f19587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model already exists\n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    # Ask the user if they want to overwrite the existing model\n",
    "    user_input = input(f\"Model already exists at {BEST_MODEL_PATH}. Do you want to overwrite it? (y/n): \")\n",
    "    if user_input.lower() == 'y':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Model already exists at {BEST_MODEL_PATH}. Please remove it before training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_epoch_activity(num_epochs, training_loader, validation_loader, model, loss_fn, optimizer, writer, patience=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model for a specified number of epochs.\n",
    "    \n",
    "    Args:\n",
    "        num_epochs: Number of epochs to train.\n",
    "        training_loader: DataLoader for the training data.\n",
    "        validation_loader: DataLoader for the validation data.\n",
    "        model: The model to train.\n",
    "        loss_fn: Loss function.\n",
    "        optimizer: Optimizer for updating model weights.\n",
    "        writer: TensorBoard SummaryWriter for logging.\n",
    "        patience: Number of epochs to wait for improvement before stopping.\n",
    "    \"\"\"\n",
    "    best_acc = 0.0 # Best validation accuracy\n",
    "    no_improve_epochs = 0  # Counter for epochs without improvement\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss, train_acc = train_epoch(model, training_loader, loss_fn, optimizer, writer, epoch)\n",
    "        val_loss, val_acc = eval_epoch(model, validation_loader, loss_fn, writer, epoch)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\", \n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\", \n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            no_improve_epochs = 0  # Reset counter if validation accuracy improves\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "        # Save the model last epoch\n",
    "        torch.save(model.state_dict(), LAST_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(MODEL_PATH+'/runs/deepfake_logs_{}'.format(timestamp))\n",
    "\n",
    "if CROSS_VAL:\n",
    "    # K-Fold Cross Validation\n",
    "    splits = NUM_SPLITS\n",
    "    kfold = KFold(n_splits=splits, shuffle=True)\n",
    "    dataset = ConcatDataset([training_set, validation_set])\n",
    "\n",
    "    for fold, (train_ids, test_ids) in tqdm(enumerate(kfold.split(dataset))):\n",
    "\n",
    "        print(f'Fold {fold+1}/{splits}')\n",
    "        training_loader = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "        validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE_VAL, shuffle=False)\n",
    "            \n",
    "        per_epoch_activity(NUM_EPOCHS, training_loader, validation_loader, model, loss_fn, optimizer, writer, patience=PATIENCE)\n",
    "else:\n",
    "    # No K-Fold Cross Validation\n",
    "    per_epoch_activity(NUM_EPOCHS, training_loader, validation_loader, model, loss_fn, optimizer, writer, patience=PATIENCE)\n",
    "    \n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6cc471",
   "metadata": {},
   "source": [
    "# Open saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = model_module.Model()\n",
    "saved_model.load_state_dict(torch.load(BEST_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511070f1",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference on a single video\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "video_path = 'data/val_sub/avoqheikrk.mp4'\n",
    "frames = extract_frames(video_path, num_frames=16)\n",
    "inputs = data_transforms(frames[0]).unsqueeze(0)  # For demonstration, use first frame\n",
    "inputs = inputs.to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(inputs.unsqueeze(1))  # shape (1, num_frames, C, H, W)\n",
    "    pred = output.argmax(dim=1).item()\n",
    "print(\"Predicted label: \", 'Fake' if pred == 1 else 'Real')\n",
    "print(\"Real label: \", 'Fake' if metadata_df[metadata_df['filename'] == 'acagallncj.mp4']['label'].values[0]== 1 else 'Real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b8ad5",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test loop on validation set\n",
    "model.eval()\n",
    "all_labels, all_preds = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "acc  = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds, average='binary')\n",
    "rec  = recall_score(all_labels, all_preds, average='binary')\n",
    "f1   = f1_score(all_labels, all_preds, average='binary')\n",
    "\n",
    "class_report = classification_report(all_labels, all_preds, labels=classes, target_names=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision     : {prec:.4f}\")\n",
    "print(f\"Recall        : {rec:.4f}\")\n",
    "print(f\"F1 Score      : {f1:.4f}\\n\")\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1344d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'filename': test_set.get_filenames(),\n",
    "    'label': all_labels,\n",
    "    'predicted_label': all_preds\n",
    "})\n",
    "results_df['correct'] = results_df['label'] == results_df['predicted_label']\n",
    "results_df.to_csv(MODEL_PATH_METRICS+'results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22240b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Acc, Precision, Recall, F1 to CSV\n",
    "metrics_df = pd.DataFrame({\n",
    "    'accuracy': [acc],\n",
    "    'precision': [prec],\n",
    "    'recall': [rec],\n",
    "    'f1_score': [f1]\n",
    "})\n",
    "metrics_df.to_csv(MODEL_PATH_METRICS+'metrics.csv', index=False)\n",
    "\n",
    "# Save classification report to txt file\n",
    "with open(MODEL_PATH_METRICS+'classification_report.txt', 'w') as f:\n",
    "    f.write(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.savefig(MODEL_PATH_METRICS+'confusion_matrix.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
